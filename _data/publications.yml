accelopt2025:
  title: "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization"
  abstract: "<p>
  We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from 49% to 61% on Trainium 1 and from 45% to 59% on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being 26x cheaper.
  </p>"
  venue: "Preprint"
  year: 2025
  month: November
  blog: "https://zhang677.github.io/blog_md/accelopt.html"
  pdf: /publications/accelopt2025.pdf
  authors:
    - genghan
    - shaowei
    - anjiang
    - zhenyu
    - allen
    - zhen
    - nandita
    - yida
    - kunle

agent2025:
  title: "Adaptive Self-improvement LLM Agentic System for ML Library Development"
  abstract: "<p>
    ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to 3.9x over a baseline single LLM.
    </p>"
  venue: "ICML 2025 Poster (acceptance rate: 26.9%); ICML 2025 ES-FoMo Workshop"
  year: 2025
  month: July
  award: "ICLR 2025 DL4C Workshop Best Paper Award (2/63)"
  blog: "https://zhang677.github.io/blog_md/aspl.html"
  pdf: /publications/agent2025.pdf
  authors:
    - genghan
    - weixin
    - olivia
    - kunle
  bibtex: 
    " @article{zhang2025adaptive,
    <br> title={Adaptive Self-improvement LLM Agentic System for ML Library Development},
    <br> author={Zhang, Genghan and Liang, Weixin and Hsu, Olivia and Olukotun, Kunle},
    <br> journal={arXiv preprint arXiv:2502.02534},
    <br> year={2025}
    <br> }"

step2025:
  title: "Streaming Tensor Program: A Streaming Abstraction for Dynamic Parallelism"
  abstract: "<p>
  Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.
  </p>"
  venue: "Preprint"
  year: 2025
  month: November
  pdf: /publications/step2025.pdf
  authors:
    - gina
    - genghan
    - konstantin
    - jungwoo
    - nathans
    - nathanz
    - olivia
    - kunle

ttt2025:
  title: "Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
  abstract: "<p>
  Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.
  </p>"
  venue: "ICML 2025 Spotlight (acceptance rate 2.59%)"
  year: 2025
  month: July
  pdf: /publications/ttt2025.pdf
  authors:
    - yusun
    - xinhao
    - karan
    - jiarui
    - arjun
    - genghan
    - yann
    - xinlei
    - xiaolong
    - sanmi
    - tatsu
    - carlos
  co_first_authors:
    - yusun
    - xinhao
    - karan

pldi2024:
  title: "Compilation of Modular and General Sparse Workspaces"
  abstract: "<p>
    Recent years have seen considerable work on compiling sparse tensor algebra
    expressions. This paper addresses a shortcoming in that work, namely how to
    generate efficient code (in time and space) that scatters values into a sparse
    result tensor. We address this shortcoming through a compiler design that
    generates code that uses sparse intermediate tensors (sparse workspaces) as
    efficient adapters between compute code that scatters and result tensors that
    do not support random insertion.  Our compiler automatically detects sparse
    scattering behavior in tensor expressions and inserts necessary intermediate
    workspace tensors. We present an algorithm template for workspace insertion
    that is the backbone of our code generation algorithm. Our algorithm template
    is modular by design, supporting sparse workspaces that span multiple
    user-defined implementations.  Our evaluation shows that sparse workspaces can
    be up to 27.12x faster than the dense workspaces of prior work. On the
    other hand, dense workspaces can be up to 7.58x faster than the sparse
    workspaces generated by our compiler in other situations, which motivates our
    compiler design that supports both. Our compiler produces sequential code that
    is competitive with hand-optimized linear and tensor algebra libraries on the
    expressions they support, but that generalizes to any other expression. Sparse
    workspaces are also more memory efficient than dense workspaces as they
    compress away zeros. This compression can asymptotically decrease memory usage,
    enabling tensor computations on data that would otherwise run out of memory.
    </p>"
  venue: "PLDI 2024 (acceptance rate 27.64%)" 
  year: 2024
  month: June
  pdf: /publications/pldi2024.pdf
  authors:
    - genghan
    - olivia
    - fred
  bibtex:
    " @article{10.1145/3656426,
   <br> author = {Zhang, Genghan and Hsu, Olivia and Kjolstad, Fredrik},
   <br> title = {Compilation of Modular and General Sparse Workspaces},
   <br> year = {2024},
   <br> issue_date = {June 2024},
   <br> publisher = {Association for Computing Machinery},
   <br> address = {New York, NY, USA},
   <br> volume = {8},
   <br> number = {PLDI},
   <br> url = {https://dl.acm.org/doi/10.1145/3656426},
   <br> doi = {10.1145/3656426},
   <br> journal = {Proc. ACM Program. Lang.},
   <br> month = {jun},
   <br> articleno = {196},
   <br> numpages = {26},
   <br> keywords = {code composition, compilation, sparse tensor algebra, sparse workspaces}
   <br> }"
  movie: https://www.youtube.com/watch?v=bkovjLkg8yI&ab_channel=ACMSIGPLAN
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/bkovjLkg8yI?si=zvYw1s6otCidWiqU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

cats2024:
  title: "CATS: Context-Aware Thresholding for Sparsity in Large Language Models"
  abstract: "<p>
    The dramatic improvements in Large Language Models (LLMs) come at the cost of increased computational resources for inference. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks.
    In this work, we introduce a new framework for sparsifying the activations of LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS).
    CATS is a relatively simple algorithm that is easy to implement and highly effective.
    At the heart of our framework is a new non-linear activation function.
    We demonstrate that CATS can be applied to various models, including Mistral-7B and Llama2-7B & 13B, and outperforms existing sparsification techniques across multiple tasks.
    More precisely, CATS-based models achieve downstream task performance within ~99% of their base models at 50% activation sparsity, even without fine-tuning.
    Moreover, with fine-tuning that targets only 1% of the parameters, CATS-based models not only converge faster but also achieve better task performance than competing techniques.
    Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation sparsity of CATS to real wall-clock time speedups.
    Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token
    generation. We release our code, experiments, and datasets at https://github.com/ScalingIntelligence/CATS.
    </p>"
  venue: "COLM 2024 (acceptance rate 28.86%)"
  year: 2024
  month: October
  blog: https://scalingintelligence.stanford.edu/blogs/cats/
  pdf: /publications/cats2024.pdf
  authors:
    - donghyun
    - jeyong
    - genghan
    - mo
    - azalia
  co_first_authors:
    - donghyun
    - jeyong
  bibtex: 
    " @inproceedings{leecats,
    <br> title={CATS: Context-Aware Thresholding for Sparsity in Large Language Models},
    <br> author={Lee, Donghyun and Lee, Jaeyong and Zhang, Genghan and Tiwari, Mo and Mirhoseini, Azalia},
    <br> booktitle={First Conference on Language Modeling},
    <br> year={2024}
    <br> }"

sgap2023:
  title: "Sgap: Towards Efficient Sparse Tensor Algebra Compilation for GPU"
  abstract: "<p>
    Sparse compiler is a promising solution for sparse tensor algebra optimization. In compiler implementation, reduction in sparse-dense hybrid algebra plays a key role in performance. Though GPU provides various reduction semantics that can better utilize the parallel computing and memory bandwidth capacity, the central question is: how to elevate the flexible reduction semantics to sparse compilation theory that assumes serial execution. Specifically, we have to tackle two main challenges: (1) there are wasted parallelism by adopting static synchronization granularity (2) static reduction strategy limits optimization space exploration. We propose Sgap: segment group and atomic parallelism to solve these problems. Atomic parallelism captures the flexible reduction semantics to systematically analyze the optimization space of sparse-dense hybrid algebra on GPU. It is a new optimization technique beyond current compiler-based and open-source runtime libraries. Segment group elevates the flexible reduction semantics to suitable levels of abstraction in the sparse compilation theory. It adopts changeable group size and user-defined reduction strategy to solve challenge (1) and (2), respectively. Finally, we use GPU sparse matrix-matrix multiplication (SpMM) on the TACO compiler as a use case to demonstrate the effectiveness of segment group in reduction semantics elevation. We achieve up to 1.2x speedup over the original TACO's SpMM kernels. We also apply new optimization techniques found by atomic parallelism to an open-source state-of-the-art SpMM library dgSPARSE. We achieve 1.6x~2.3x speedup on the algorithm tuned with atomic parallelism.
    </p>"
  venue: "CCF Transactions on High Performance Computing"
  year: 2023
  pdf: /publications/sgap2023.pdf
  authors:
    - genghan
    - yuetong
    - yanting
    - zhongming
    - guohao
    - sitao
    - yuan
    - pavlos
    - yu
  bibtex: 
    " @article{zhang2023sgap,
    <br> title={Sgap: towards efficient sparse tensor algebra compilation for GPU},
    <br> author={Zhang, Genghan and Zhao, Yuetong and Tao, Yanting and Yu, Zhongming and Dai, Guohao and Huang, Sitao and Wen, Yuan and Petoumenos, Pavlos and Wang, Yu},
    <br> journal={CCF Transactions on High Performance Computing},
    <br> volume={5},
    <br> number={2},
    <br> pages={210--227},
    <br> year={2023},
    <br> publisher={Springer}
    <br> }"