agent2025:
  title: "Adaptive Self-improvement LLM Agentic System for ML Library Development"
  abstract: "<p>
    ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to 3.9x over a baseline single LLM.
    </p>"
  venue: "ICLR 2025 Workshop on Reasoning and Planning for Large Language Models; ICLR 2025 DL4C Workshop"
  year: 2025
  month: May
  award: "ICLR 2025 DL4C Workshop Best Paper Award (2/63)"
  blog: "https://zhang677.github.io/blog_md/aspl.html"
  pdf: /publications/agent2025.pdf
  authors:
    - genghan
    - weixin
    - olivia
    - kunle
  bibtex: 
    " @article{zhang2025adaptive,
    <br> title={Adaptive Self-improvement LLM Agentic System for ML Library Development},
    <br> author={Zhang, Genghan and Liang, Weixin and Hsu, Olivia and Olukotun, Kunle},
    <br> journal={arXiv preprint arXiv:2502.02534},
    <br> year={2025}
    <br> }"

pldi2024:
  title: "Compilation of Modular and General Sparse Workspaces"
  abstract: "<p>
    Recent years have seen considerable work on compiling sparse tensor algebra
    expressions. This paper addresses a shortcoming in that work, namely how to
    generate efficient code (in time and space) that scatters values into a sparse
    result tensor. We address this shortcoming through a compiler design that
    generates code that uses sparse intermediate tensors (sparse workspaces) as
    efficient adapters between compute code that scatters and result tensors that
    do not support random insertion.  Our compiler automatically detects sparse
    scattering behavior in tensor expressions and inserts necessary intermediate
    workspace tensors. We present an algorithm template for workspace insertion
    that is the backbone of our code generation algorithm. Our algorithm template
    is modular by design, supporting sparse workspaces that span multiple
    user-defined implementations.  Our evaluation shows that sparse workspaces can
    be up to 27.12x faster than the dense workspaces of prior work. On the
    other hand, dense workspaces can be up to 7.58x faster than the sparse
    workspaces generated by our compiler in other situations, which motivates our
    compiler design that supports both. Our compiler produces sequential code that
    is competitive with hand-optimized linear and tensor algebra libraries on the
    expressions they support, but that generalizes to any other expression. Sparse
    workspaces are also more memory efficient than dense workspaces as they
    compress away zeros. This compression can asymptotically decrease memory usage,
    enabling tensor computations on data that would otherwise run out of memory.
    </p>"
  venue: "Conference on Programming Language Design and Implementation (PLDI)" 
  year: 2024
  month: June
  pdf: /publications/pldi2024.pdf
  authors:
    - genghan
    - olivia
    - fred
  bibtex:
    " @article{10.1145/3656426,
   <br> author = {Zhang, Genghan and Hsu, Olivia and Kjolstad, Fredrik},
   <br> title = {Compilation of Modular and General Sparse Workspaces},
   <br> year = {2024},
   <br> issue_date = {June 2024},
   <br> publisher = {Association for Computing Machinery},
   <br> address = {New York, NY, USA},
   <br> volume = {8},
   <br> number = {PLDI},
   <br> url = {https://dl.acm.org/doi/10.1145/3656426},
   <br> doi = {10.1145/3656426},
   <br> journal = {Proc. ACM Program. Lang.},
   <br> month = {jun},
   <br> articleno = {196},
   <br> numpages = {26},
   <br> keywords = {code composition, compilation, sparse tensor algebra, sparse workspaces}
   <br> }"
  movie: https://www.youtube.com/watch?v=bkovjLkg8yI&ab_channel=ACMSIGPLAN
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/bkovjLkg8yI?si=zvYw1s6otCidWiqU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>



