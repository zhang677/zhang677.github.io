agent2025:
  title: "Adaptive Self-improvement LLM Agentic System for ML Library Development"
  abstract: "<p>
    ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to 3.9x over a baseline single LLM.
    </p>"
  venue: "ICLR 2025 Workshop on Reasoning and Planning for Large Language Models; ICLR 2025 DL4C Workshop"
  year: 2025
  month: May
  award: "ICLR 2025 DL4C Workshop Best Paper Award (2/63)"
  blog: "https://zhang677.github.io/blog_md/aspl.html"
  pdf: /publications/agent2025.pdf
  authors:
    - genghan
    - weixin
    - olivia
    - kunle
  bibtex: 
    " @article{zhang2025adaptive,
    <br> title={Adaptive Self-improvement LLM Agentic System for ML Library Development},
    <br> author={Zhang, Genghan and Liang, Weixin and Hsu, Olivia and Olukotun, Kunle},
    <br> journal={arXiv preprint arXiv:2502.02534},
    <br> year={2025}
    <br> }"

pldi2024:
  title: "Compilation of Modular and General Sparse Workspaces"
  abstract: "<p>
    Recent years have seen considerable work on compiling sparse tensor algebra
    expressions. This paper addresses a shortcoming in that work, namely how to
    generate efficient code (in time and space) that scatters values into a sparse
    result tensor. We address this shortcoming through a compiler design that
    generates code that uses sparse intermediate tensors (sparse workspaces) as
    efficient adapters between compute code that scatters and result tensors that
    do not support random insertion.  Our compiler automatically detects sparse
    scattering behavior in tensor expressions and inserts necessary intermediate
    workspace tensors. We present an algorithm template for workspace insertion
    that is the backbone of our code generation algorithm. Our algorithm template
    is modular by design, supporting sparse workspaces that span multiple
    user-defined implementations.  Our evaluation shows that sparse workspaces can
    be up to 27.12x faster than the dense workspaces of prior work. On the
    other hand, dense workspaces can be up to 7.58x faster than the sparse
    workspaces generated by our compiler in other situations, which motivates our
    compiler design that supports both. Our compiler produces sequential code that
    is competitive with hand-optimized linear and tensor algebra libraries on the
    expressions they support, but that generalizes to any other expression. Sparse
    workspaces are also more memory efficient than dense workspaces as they
    compress away zeros. This compression can asymptotically decrease memory usage,
    enabling tensor computations on data that would otherwise run out of memory.
    </p>"
  venue: "Conference on Programming Language Design and Implementation (PLDI)" 
  year: 2024
  month: June
  pdf: /publications/pldi2024.pdf
  authors:
    - genghan
    - olivia
    - fred
  bibtex:
    " @article{10.1145/3656426,
   <br> author = {Zhang, Genghan and Hsu, Olivia and Kjolstad, Fredrik},
   <br> title = {Compilation of Modular and General Sparse Workspaces},
   <br> year = {2024},
   <br> issue_date = {June 2024},
   <br> publisher = {Association for Computing Machinery},
   <br> address = {New York, NY, USA},
   <br> volume = {8},
   <br> number = {PLDI},
   <br> url = {https://dl.acm.org/doi/10.1145/3656426},
   <br> doi = {10.1145/3656426},
   <br> journal = {Proc. ACM Program. Lang.},
   <br> month = {jun},
   <br> articleno = {196},
   <br> numpages = {26},
   <br> keywords = {code composition, compilation, sparse tensor algebra, sparse workspaces}
   <br> }"
  movie: https://www.youtube.com/watch?v=bkovjLkg8yI&ab_channel=ACMSIGPLAN
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/bkovjLkg8yI?si=zvYw1s6otCidWiqU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

sgap2023:
  title: "Sgap: Towards Efficient Sparse Tensor Algebra Compilation for GPU"
  abstract: "<p>
    Sparse compiler is a promising solution for sparse tensor algebra optimization. In compiler implementation, reduction in sparse-dense hybrid algebra plays a key role in performance. Though GPU provides various reduction semantics that can better utilize the parallel computing and memory bandwidth capacity, the central question is: how to elevate the flexible reduction semantics to sparse compilation theory that assumes serial execution. Specifically, we have to tackle two main challenges: (1) there are wasted parallelism by adopting static synchronization granularity (2) static reduction strategy limits optimization space exploration. We propose Sgap: segment group and atomic parallelism to solve these problems. Atomic parallelism captures the flexible reduction semantics to systematically analyze the optimization space of sparse-dense hybrid algebra on GPU. It is a new optimization technique beyond current compiler-based and open-source runtime libraries. Segment group elevates the flexible reduction semantics to suitable levels of abstraction in the sparse compilation theory. It adopts changeable group size and user-defined reduction strategy to solve challenge (1) and (2), respectively. Finally, we use GPU sparse matrix-matrix multiplication (SpMM) on the TACO compiler as a use case to demonstrate the effectiveness of segment group in reduction semantics elevation. We achieve up to 1.2x speedup over the original TACO's SpMM kernels. We also apply new optimization techniques found by atomic parallelism to an open-source state-of-the-art SpMM library dgSPARSE. We achieve 1.6x~2.3x speedup on the algorithm tuned with atomic parallelism.
    </p>"
  venue: "CCF Transactions on High Performance Computing"
  year: 2023
  pdf: /publications/sgap2023.pdf
  authors:
    - genghan
    - yuetong
    - yanting
    - zhongming
    - guohao
    - sitao
    - yuan
    - pavlos
    - yu
  bibtex: 
    " @article{zhang2023sgap,
    <br> title={Sgap: towards efficient sparse tensor algebra compilation for GPU},
    <br> author={Zhang, Genghan and Zhao, Yuetong and Tao, Yanting and Yu, Zhongming and Dai, Guohao and Huang, Sitao and Wen, Yuan and Petoumenos, Pavlos and Wang, Yu},
    <br> journal={CCF Transactions on High Performance Computing},
    <br> volume={5},
    <br> number={2},
    <br> pages={210--227},
    <br> year={2023},
    <br> publisher={Springer}
    <br> }"

cats2024:
  title: "CATS: Context-Aware Thresholding for Sparsity in Large Language Models"
  abstract: "<p>
    The dramatic improvements in Large Language Models (LLMs) come at the cost of increased computational resources for inference. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks.
    In this work, we introduce a new framework for sparsifying the activations of LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS).
    CATS is a relatively simple algorithm that is easy to implement and highly effective.
    At the heart of our framework is a new non-linear activation function.
    We demonstrate that CATS can be applied to various models, including Mistral-7B and Llama2-7B & 13B, and outperforms existing sparsification techniques across multiple tasks.
    More precisely, CATS-based models achieve downstream task performance within ~99% of their base models at 50% activation sparsity, even without fine-tuning.
    Moreover, with fine-tuning that targets only 1% of the parameters, CATS-based models not only converge faster but also achieve better task performance than competing techniques.
    Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation sparsity of CATS to real wall-clock time speedups.
    Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token
    generation. We release our code, experiments, and datasets at https://github.com/ScalingIntelligence/CATS.
    </p>"
  venue: "First Conference on Language Modeling"
  year: 2024
  month: October
  blog: https://scalingintelligence.stanford.edu/blogs/cats/
  pdf: /publications/cats2024.pdf
  authors:
    - donghyun
    - jaeyong
    - genghan
    - mo
    - azalia
  bibtex: 
    " @inproceedings{leecats,
    <br> title={CATS: Context-Aware Thresholding for Sparsity in Large Language Models},
    <br> author={Lee, Donghyun and Lee, Jaeyong and Zhang, Genghan and Tiwari, Mo and Mirhoseini, Azalia},
    <br> booktitle={First Conference on Language Modeling},
    <br> year={2024}
    <br> }"