---
layout: blog
title: Adaptive Self-improvement LLM Agentic System for ML Library Development
date: 2025-03-01
authors:
  - genghan
  - weixin
  - olivia
  - kunle
---

# ML library development using ASPLs

ML libraries, such as cuDNN and MIOpen, provide high-performance implementation of the operators in ML frameworks such as PyTorch and Jax. They are key to efficient ML systems. ML libraries are often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures (DSAs). Figure 1 shows an example of ML library development.

<div class="figure">
  <img src="/assets/img/step_intro.png" alt="Alt text describing the image">
  <div class="caption">
    <strong>Figure 1:</strong> STeP is an ASPL that abstracts the execution on a DSA called reconfigurable dataflow architecture. As an analogy, STeP for reconfigurable dataflow architecture is like CUDA for NVIDIA GPUs. As a task of ML library development, a simplified mixture-of-expert (MoE) module is implemented using STeP.
  </div>
</div>


However, writing these high-performance ML libraries is challenging because it needs expert knowledge of ML algorithms and the ASPL. This process requires complex reasoning to compose high-level ML model components (“operators”) with low-level hardware primitives. Meanwhile, ASPLs are updated per generation of DSA, so there is limited data as shown in Figure 2.

<div class="figure">
  <img src="/assets/img/motivation.png" alt="Alt text describing the image">
  <div class="caption">
    <strong>Figure 2:</strong> ML library requires complex reasoning while minimizing data requirements. <a href="https://github.com/NVIDIA/cutlass/tree/main">CUTLASS</a> is an ASPL for developing ML libraries on NVIDIA GPUs with tensor cores.
  </div>
</div>

# Adaptive Self-improvement LLM Agentic System

Large language models (LLMs) have shed light on solving complex programming tasks. This work explores LLMs’ potential for automating ML library development. We propose an adaptive self-improvement learning algorithm integrated with an agentic system organization that orchestrates LLMs with different capabilities. Figure 3 shows the system diagram.

<div class="figure">
  <img src="/assets/img/intro.png" alt="Alt text describing the image">
  <div class="caption">
    <strong>Figure 3:</strong> LLM agents start from their base knowledge and accumulate experiences through parallel sampling. Our adaptive self-improvement learning algorithm filters high-quality answers, stratifies the earned experiences by difficulty, and adaptively selects demonstrations to enhance LLM agents.
  </div>
</div>


We find that adaptive self-improvement can elevate weaker base models above powerful reasoning models; self-improved DeepSeek-V3 completes 3.9x more tasks than a single DeepSeek-V3 and outperforms OpenAI-o1. Adaptively selecting a smaller number of high-quality demonstrations can outperform larger quantities of lower-quality demonstrations. Interestingly, Claude 3.5 Sonnet unexpectedly surpasses other models, including self-improved DeepSeek-V3.


Adaptive self-improvement learning evolves LLM agents with data generated by themselves. As shown in Figure 4, LLM agents accumulate experience through sampling on each task, with experiences becoming more hard-earned as success rates decrease. Our algorithm prioritizes hard-earned experiences as demonstrations. When these hard-earned experiences are used up, the algorithm adaptively increases the number of demonstrations by mixing experiences earned from less challenging tasks. Such experience stratification and adaptive selection is inspired by curriculum learning<sup>1</sup>. As a byproduct, the algorithm adaptively increases test-time compute on challenging tasks until they are finished or the data is used up.

<div class="figure">
  <img src="/assets/img/self-improvement.gif" alt="Alt text describing the image">
  <div class="caption">
    <strong>Figure 4:</strong> White, green, and orange circles represent unfinished tasks, finished tasks and selected experiences, respectively. Experiences are stratified into 3 levels: hard, medium, and easy based on the number of correct samples. 
  </div>
</div>

# References

<ol>
  <li id="ref1"><a href="#ref1">Bengio, Y., Louradour, J., Collobert, R. and Weston, J., 2009, June. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).</a></li>
</ol>